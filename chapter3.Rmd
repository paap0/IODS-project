

---
title: ""
output:
  html_document:
    toc: true
    toc_depth: 4
---

## **3<sup>rd</sup> WEEK**: Logistic regression

### Introduction

Data including grades, demographic, social and school related features were collected in two Portuguese schools using school reports and questionnaires . There are two separate datasets regarding performance in distinct subjects, namely Mathematics and Portuguese.

The original data of the analysis in this exercise are freely available as a [zip file](https://archive.ics.uci.edu/ml/machine-learning-databases/00320/). Additional  [metadata](https://archive.ics.uci.edu/ml/datasets/Student+Performance) information describes basic characteristics of the data set.

The original data sets needed to be joined and edited for the analysis according to [this R script](https://github.com/paap0/IODS-project/blob/master/data/create_alc.R). The variables not used for joining the two data were combined by averaging them. An additional variable *high_use* was created by taking the average of the sum of alcohol consumption during weekdays and weekends, which was thereafter further modified to yield a logical TRUE or FALSE *high_use* variable if greater than 2 weekly proportions or not, respectively. 


```{r include=FALSE, cache=FALSE}
#Define packages required 
library(dplyr)
library(GGally)
library(ggplot2)
library(tableone)
library(tidyr)
library(tibble)
library(magrittr)
library(boot)
library(stargazer)
library(gmodels)
#Multiplot
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


### Preliminary information and descriptive statistics

#### Basic characteristics of the dataset

Data is read in and the structure of the dataset is checked.
```{r}
# Read in the dataset
alc<-read.csv(file="alc.csv", header=TRUE)
# Inspect the structure
glimpse(alc)
```


The final data set includes 382 respondents and 35 both numerical (5 scaled) and factorial variables.

```{r}
#Names of the variables
colnames(alc)
```


The purpose is to investigate the relationship between TRUE/FALSE *high alcohol consumption* and four chosen independent variables. Based on assumed impact and personal interest they are:



 Variable Name | Explanation | Classification  
--- | --- | --- | --- |---
**<span style="color:red">SEX</span>**| Gender | factorial | binary | F-female/M-male|
**<span style="color:green">ABSENCES</span>**| Number of school abscenses | integer | 0-93 |hours|
**<span style="color:blue">FAMREL</span>**| Quality of family relationships | integer | 1-5 |Very bad - Very good|
**<span style="color:orange">HEALTH</span>**| Current health status | integer | 1-5 |Very bad - Very good|





#### Hypotheses

Briefly, I assume that men drink more, more school absences are associated with drinking habits, students with worse family relationships drink more, and better health is associated with less alcohol consumption.
Specifically stated hypotheses are: **H1: <span style="color:red">*Males are more prone to belong to high users of alcohol defined by more than 2 proportions a week than females*</span> H2: <span style="color:green">*Students with more school absencies drink more than those attending school more frequently.*</span> H3: <span style="color:blue">*Students with very good family relationships consume less alcohol than do students with bad family relationships.*</span>  H4: <span style="color:orange">*Very good health status is protective for high alcohol consumption (more than 2 weekly proportions) compared to very bad health status.*</span>**




#### Variable selection and summaries

Descriptive statistics are shown as a summary grouped by *high_use* to get a preliminary idea of the data set.In the package tableone the command CreateTableOne simultanously does group comparisons.

```{r echo=FALSE, results='asis'}
#Subset of the preliminary dataset
alc_study<-alc[c("high_use","sex","absences","famrel","health")]
```


```{r}
#Summaries

CreateTableOne(vars=c("sex","absences","famrel","health"), strata=c("high_use"),factorVars=c("sex"),data=alc_study )

```

There are more high alcohol user males than there are females. The mean absence scores are higher among the high users and the mean score for family relationships is lower among them. However, the mean health score among the high users is slightly higher for the high users than for the non-high-users, which observation differs from the preliminary assumption. 



#### Graphical overviews


Let´s take a look at the variables graphically. To have multiple figures on the same plot I use the great [multiplot]("http://www.peterhaschke.com/r/2013/04/24/MultiPlot.html") script.

```{r} 

```

```{r}
#Matrix of plots
ggpairs(alc_study[-1], mapping = aes(col = sex), lower = list(combo = wrap("facethist", bins = 20)), title="Graphical overview of the 4 variables")


p1 <- ggplot(alc_study, aes(sex)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('Gender')+ggtitle("Gender") +scale_fill_manual(values=c("lightblue","red"))


p2 <- ggplot(alc_study, aes(x=high_use,y= absences,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Absences (means included)")+
scale_fill_manual(values=c("lightblue","red"))


p3 <- ggplot(alc_study, aes(x=high_use,y= famrel,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Family relationships (means included)")+
scale_fill_manual(values=c("lightblue","red"))


p4 <- ggplot(alc_study, aes(x=high_use,y= health,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Health status (means included)")+
scale_fill_manual(values=c("lightblue","red"))


multiplot(p1, p2, p3, p4, cols = 2)

```

A greater proportion of males belong to the *high_use* group than of females. The mean value for the quality of family relationships is higher for the ones not using more than average amount of alcohol (4.00 vs 3.78). However, there are no differencies in the median values (4.00 vs 4.00). Both the  mean (6.37) and median (4.00) values for absences are higher in the higher usage group than in the group of students consuming less than average amount (3.00,3.71). There are also quite a few outliers in both groups. Surprisingly, the high-user group members have higher current health scores (mean=3.70, median=4.00) than do the non-high-user ones (3.52,4.00). 

Thus, based on both the summary statistics and the graphical overview it seems that all of the variables except health are at least to some extent associated with the level of alcohol consumption. Further, there are no relevant correlations between the chosen variables. To further explore the associations a logistic regression analysis is carried out.



### Fitting the model
  
A logistic regression model with four explanatory variables selected based on the preliminary interest is fitted to identify factors related to *higher than average student alcohol consumption. investigate the relationship between them and the binary target variable*  Logistic regression is basic approach for binary outcomes and its results provide probability estimates of the event happening.

```{r}
#First model with four explanatory variables
m1<-glm(high_use~sex+absences+famrel+health,data=alc_study,family="binomial")
summary(m1)
```

The summary of the model confirm what could already be suspected based on the preliminary grouped summaries and plots.
The variables sex, absences and famrel are significant at the 5 % level. The effect of health was not even borderline significant (p=0.3426).  

```{r}
OR<-coef(m1) %>% exp()
CI<-confint(m1) %>% exp()
cbind(OR, CI)
```


To better understand the summary output and its interpretations two terms should be understood: *Odds* is the probability divided by the opposite of that probability. Further, the ratio of two odds is called *the odds ratio*. Odds ratio can be used to quantify the relationship between an explanatory factor and the target variable. Odds higher than 1 mean that the factor is positively associated with event happening, and odds less than 1 refer to a negative, or, if the event is unwanted (e.g. death), a protective effect.
In the logistic regression model the target variable is not purely odds, but the lof of odds. Thus, ecponents of the coefficient can be interpreted as odds ratios bwteeen a unit change in the cosserponding explanatory variable

Confidence intervals for the odds ratio for health includes 1, as assumed.  

Next, the non-significant variable health is excluded and the model is fitted again. The odds are obtained by exponentiating the estimates and profile likelihood-based confidence intervals by using the confint command from MASS package:

```{r}
#First model with four explanatory variables
m2<-glm(high_use~sex+absences+famrel,data=alc_study,family="binomial")
```

```{r}
cbind(exp(coef(m2)),exp(confint(m2)))
```

Now all of the variables in the model are significant at the 5% level. E.g. an increase of one unit in the student's school absences  increases his/her log-odds to be a high alcohol consumer by 0.09. The confidence interval can be constructed using normal approximation for the parameter estimate: \hat{\beta}_i \pm z^{\alpha/2}SE(\hat{\beta}_i) . The 95% confidence interval is (0.009, 0.037). 

The significant odds ratios reveal that males are 2.8 times more likely to be high consumers of alcohol than females and that with increasing absences the person is more likely to be a high consumer. On the contrary, the better the family relationships the less likely the person is to be a high consumer of alcohol.

The coefficient for female= 0.59278 which corresponds to the log of odds ratio between the female group and male group. The odds ratio equals 1.81 which means the odds for females are about 81% higher than the odds for males.

The coefficient for math= 0.15634 which is interpreted as the expected change in log odds for a one-unit increase in the math score. The odds ratio can be calculated by exponentiating this value to get 1.16922 which means we expect to see about 17% increase in the odds of being in an honors class, for a one-unit increase in math score

```{r, results='asis'}
stargazer(m2, header=FALSE, type='html')
```

The second model will be used to explore the predictive power:

```{r}
probabilities<-predict(m2,type="response")
predictions<-probabilities >0.5
# Add the probabilities
alc_study <- mutate(alc_study, probability = probabilities)
# Calculate a logical high use value based on probabilites.
alc_study <- mutate(alc_study, prediction = probability > 0.5)
table(high_use=alc_study$high_use,prediction=predictions) %>% addmargins %>% round(digits=2)
table(high_use=alc_study$high_use,prediction=predictions) %>% prop.table %>% addmargins() %>% round(digits=2)
```

114 predictions out of 382 were wrong. There are altogether 256 true negatives and 29 true positives, 85 false negatives and 12 false positives. To conclude, the model predicts high consumption less frequently than it really is as can be demonstrated graphically:

```{r}
hu <- as.data.frame(prop.table(table(alc_study$high_use)))
pred <- as.data.frame(prop.table(table(alc_study$prediction)))
pp1 <- ggplot(hu, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('OBSERVED high use') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","darkred"))+
 theme(
plot.title = element_text(color="green", size=14, face="bold.italic"),
axis.title.x = element_text(color="green", size=14, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

pp2 <- ggplot(pred, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('PREDICTED high use') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","darkred"))+
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="red", size=14, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

multiplot(pp1, pp2, cols = 2)

```

Since we know how to make predictions with our model, we can also compute the average number of incorrect predictions. In our model it is 25.4% suggesting that the model is lacking accuracy.Logistic regression aims to minimize the incorrectly classified observations.

```{r}
#loss_func from DataCamp exercises
loss_func<-function(class,prob){
  n_wrong<-abs(class-prob)>0.5
  mean(n_wrong)
}
loss_func(class = alc_study$high_use, alc_study$probability)
```

### Cross validation

Cross-validation tests the model on unseen data, i.e. data not used for generating the model. The lower the value the more accurate the model. Cross-validation can also be used to compare models.

```{r}
#loss_func from DataCamp exercises to define the loss (penalty) function
loss_func<-function(class,prob){
  n_wrong<-abs(class-prob)>0.5
  mean(n_wrong)
}
#Errors computed and stored
cv <- cv.glm(data = alc_study, cost = loss_func, glmfit = m2, K = 10)
cv$delta[1] # Print the average number of wrong predictions.
```

A ten-fold cross-validation shows that on average 25.9% of the observations are missclassified under our model with three explanatory variables: sex, famrel and absencees. The average number of wrong predictions in the cross validation is only slightly better than in the DataCamp exercise. The model could be improved further by testing more variables.


Ref. Fabio Pagnotta's and Hossain Mohammad Amran's Using Data Mining To Predict Secondary School Student Alcohol Consumption (2008), published by Department of Computer Science of the University of Camerino
