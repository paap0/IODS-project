

---
title: ""
output:
  html_document:
    toc: true
    toc_depth: 4
---

## **3<sup>rd</sup> WEEK**: Logistic regression

### Introduction
Data including grades, demographic, social and school related features were collected in two Portuguese schools using school reports and questionnaires and stored as two separate datasets regarding performance in distinct subjects, namely Mathematics and Portuguese.

The original data of the analysis in this exercise are freely available as a [zip file](https://archive.ics.uci.edu/ml/machine-learning-databases/00320/). Additional  [metadata](https://archive.ics.uci.edu/ml/datasets/Student+Performance) information describes basic characteristics of the data sets. For the purporse of this particular exercise they needed to be joined and edited according to [this R script](https://github.com/paap0/IODS-project/blob/master/data/create_alc.R). The variables not used for joining the two data were combined by averaging them. An additional variable *high_use* was created by taking the average of the sum of alcohol consumption during weekdays and weekends, which was thereafter further modified to yield a logical TRUE or FALSE *high_use* variable. A treshold value for higher than average alcohol consumption was chosen to be more than 2 weekly proportions.



```{r include=FALSE, cache=FALSE}
#Define packages required 
library(dplyr)
library(GGally)
library(ggplot2)
library(tableone)
library(tidyr)
library(tibble)
library(magrittr)
library(boot)
library(stargazer)
library(gmodels)
library(MASS)
library(leaps)
library(bestglm)
#Multiplot
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


### Preliminary information and descriptive statistics

#### Basic characteristics of the dataset

Data set is read in and the structure of the dataset is checked.
```{r include=FALSE, cache=FALSE}
# Read in the dataset
alc<-read.csv(file="alc.csv", header=TRUE)
```

```{r}
# Inspect the structure
glimpse(alc)
```



The final data set includes 382 respondents and 35 both integer and factorial variables. The names of the variables are listed below (explanations can be found   [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance)
:

```{r}
#Names of the variables
colnames(alc)
```

#### Variable selection


```{r echo=FALSE, results='asis'}
#Subset of the preliminary dataset
alc_study<-alc[c("high_use","sex","absences","famrel","health")]
```

The purpose is to investigate the relationship between *high alcohol consumption* and four chosen independent variables. Based on assumed impact and personal interest they are:


 Variable Name | Explanation | Classification  
--- | --- | --- | --- |---
**<span style="color:red">SEX</span>**| Gender | factorial | binary | F-female/M-male|
**<span style="color:green">ABSENCES</span>**| Number of school abscenses | integer | 0-93 |hours|
**<span style="color:blue">FAMREL</span>**| Quality of family relationships | integer | 1-5 |Very bad - Excellent|
**<span style="color:orange">HEALTH</span>**| Current health status | integer | 1-5 |Very bad - Very good|





#### Hypotheses

Briefly, I assume that men drink more, more school absences are associated with drinking habits, students with worse family relationships drink more, and better health is associated with less alcohol consumption.
Specifically stated hypotheses are: **<span style="color:red">*Males are more prone to belong to high users of alcohol defined by more than 2 proportions a week than females*</span> <span style="color:green">*Students with more school absencies drink more than those attending school more frequently.*</span>  <span style="color:blue">*Students with very good family relationships consume less alcohol than do students with bad family relationships.*</span>  <span style="color:orange">*Very good health status is protective for high alcohol consumption (more than 2 weekly proportions) compared to very bad health status.*</span>**



#### Graphical overviews


Let´s take the first look graphically: distributions and correlations as well as suitable bar and box plots grouped by *high alcohol usage* to better understand the relationship between the chosen variables and the outcome. 
To have multiple figures on the same plot I use the great [multiplot](http://www.peterhaschke.com/r/2013/04/24/MultiPlot.html) script.


```{r}
#Matrix of plots
ggpairs(alc_study[-1], mapping = aes(col = sex), lower = list(combo = wrap("facethist", bins = 20)), title="Graphical overview of the 4 variables")
```

```{r}
p1 <- ggplot(alc_study, aes(sex)) + geom_bar(aes(fill = high_use), position = "dodge", stat="count") + xlab('Gender')+ggtitle("Gender") +scale_fill_manual(values=c("lightblue","red"))+
  theme(plot.title = element_text(size=9))



p2 <- ggplot(alc_study, aes(x=high_use,y= absences,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Absences (means included)")+
scale_fill_manual(values=c("lightblue","red"))+
theme(plot.title = element_text(size=9))


p3 <- ggplot(alc_study, aes(x=high_use,y= famrel,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Family relationships (means included)")+
scale_fill_manual(values=c("lightblue","red"))+
theme(plot.title = element_text(size=9))




p4 <- ggplot(alc_study, aes(x=high_use,y= health,fill=high_use)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
stat_summary(fun.y=mean, geom="point", shape=23, size=4)+ggtitle("Health status (means included)")+
scale_fill_manual(values=c("lightblue","red"))+
  theme(plot.title = element_text(size=9))



multiplot(p1, p2, p3, p4, cols = 2)

```

#### Grouped summary statistics and preliminary comments

In addition to data visualization, descriptive statistics are shown numerically as a summary grouped by *high_use* to get a preliminary, quantitative idea of the data set.In the package tableone the command CreateTableOne simultanously does group comparisons.


```{r}
#Summary

CreateTableOne(vars=c("sex","absences","famrel","health"), strata=c("high_use"),factorVars=c("sex"),data=alc_study )

```

There are proportionally more male high users than there are females. Family relationships on average are better for the ones not using a lot of alcohol (4.00 vs 3.78). However, there are no differencies in the median values (4.00 vs 4.00). Both the  mean (6.37) and median (4.00) values for absences are higher in the higher usage group than in the group of students consuming less than average amount (3.00,3.71). There are also quite a few outliers in both groups. Surprisingly, the high-user group members have higher current health scores (mean=3.70, median=4.00) than do the non-high-user ones (3.52,4.00). Yet, the differences are small.

Thus, based on both the graphical and numerical overviews it seems that all of the variables except health are at least to some extent associated with the level of alcohol consumption as I hypotesized. Additionally, it has been investigated that there are no relevant correlations between the chosen variables. To further explore the associations a logistic regression analysis is carried out.



### Fitting the model
  
  
A logistic regression model with four explanatory variables selected based on the preliminary interest is fitted to identify the ones related to *higher than average student alcohol consumption*. Logistic regression is basic approach for binary outcomes and its results provide probability estimates of the event happening.

```{r}
#First model with four explanatory variables
m1<-glm(high_use~sex+absences+famrel+health,data=alc_study,family="binomial")
summary(m1)
```

The summary of the model confirm what could already be suspected based on the plot and numerical grouped summary.
The variables sex, absences and famrel are significant at the 5 % level. The effect of health is not even borderline significant (p=0.3426).  From the estimates it can also be captured that the correlation between family relationships and *high use* is negative and the correlation between school absences and *high use* is positive. Furthermore, being a male positively correlates with the outcome, i.e. increases the risk of being a high consumer of alcohol.


#### Odds, odds ratio and confidence intervals

To better understand the odds ratio which is used in logistic regression to quantify the relationship between an explanatory factor and the target variabe the term *odds* should be clarified. It is the probability divided by the opposite of that probability. Logically, the ratio of two odds is called *the odds ratio*.Odds higher than 1 mean that the factor is positively associated with event happening, and odds less than 1 refers to a negative, or, if the event is unwanted (e.g. death), a protective effect.
To further complicate the matter, based on the primary equation, in the logistic regression model the target variable is not purely odds, but the log of odds. Thus, in the following table, firstly the odds are obtained by exponentiating the summary estimates and, secondly the profile likelihood-based confidence intervals are calculated by using the confint command from MASS package:

```{r include=FALSE, cache=FALSE}
OR<-coef(m1) %>% exp()
CI<-confint(m1) %>% exp()
```

```{r}
cbind(OR, CI)
```


Apart from the variable health, the confidence intervals do not include one, as suspected, referring to a statistically significant factor. If any given confidence interval would include 1 it would mean that the factor could have either a positive or a negative effect, or no effect at all on the risk of the outcome.

#### Second model

Next, the non-significant variable health is excluded and the model is fitted again. 

```{r}
#Second model with three explanatory variables
m2<-glm(high_use~sex+absences+famrel,data=alc_study,family="binomial")
summary(m2)
```

Now all of the variables in the model are significant at the 5 % level.  

```{r include=FALSE, cache=FALSE}
Oddsratios_Confint<-cbind(exp(coef(m2)),exp(confint(m2)))
```
```{r}
Oddsratios_Confint
```

The significant odds ratios reveal that the factorial variable (male) denoting the difference between the genders refers to a 2.8 (1.7 - 4.5) times higher odds for males to be high consumers of alcohol than females. The coefficient for males would actually be the intercept plus the estimate for gender. With regard to continuous variables the interpretation differs: increasing absences the person is more likely to be a high consumer. And on the contrary, the better the family relationships the less likely the person is to be a high consumer of alcohol.

More specifically, to explain and interpret both the summary and the table with odds ratios and condidence intervals I take a more detailed look at the coefficient for absences, which is 0.09352. It can be interpreted as the expected change in log odds for a one-unit increase in the number of absences. The odds ratio can be calculated by exponentiating this value to get 1.0980288 which means that we expect to see about 10% increase in the odds of being in a high alcohol user, for a one-unit increase in absences.


### Predictive power

The second model will be used to explore the predictive power:

```{r}
probabilities<-predict(m2,type="response")
predictions<-probabilities >0.5
# Add the probabilities
alc_study <- mutate(alc_study, probability = probabilities)
# Calculate a logical high use value based on probabilites.
alc_study <- mutate(alc_study, prediction = probability > 0.5)
table(high_use=alc_study$high_use,prediction=predictions) %>% addmargins %>% round(digits=2)
table(high_use=alc_study$high_use,prediction=predictions) %>% prop.table %>% addmargins() %>% round(digits=2)
```

There are 256 true negatives and 29 true positives, 85 false negatives and 12 false positives. To conclude, the model predicts high consumption less frequently than it really is as can be demonstrated graphically:

```{r}
hu <- as.data.frame(prop.table(table(alc_study$high_use)))
pred <- as.data.frame(prop.table(table(alc_study$prediction)))
pp1 <- ggplot(hu, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('OBSERVED high use') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","green"))+
 theme(
plot.title = element_text(color="green", size=14, face="bold.italic"),
axis.title.x = element_text(color="green", size=9, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

pp2 <- ggplot(pred, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('PREDICTED high use') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","orange"))+
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="orange", size=8, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

multiplot(pp1, pp2, cols = 2)

```

Since we know how to make predictions with our model, we can also compute the average number of incorrect predictions. In our model it is 25.4% suggesting that the model is lacking accuracy.Logistic regression aims to minimize the incorrectly classified observations. However, the performance of the generated model is better than purely guessing or just tossing a coin, which has the probability of 50 % of predicting the one or another outcome.

```{r}
#loss_func from DataCamp exercises
loss_func<-function(class,prob){
  n_wrong<-abs(class-prob)>0.5
  mean(n_wrong)
}
loss_func(class = alc_study$high_use, alc_study$probability)#Training error of the three variable model
```


### Cross validation

Cross-validation tests the model on unseen data, i.e. data not used for generating the model. The lower the value the more accurate the model. Cross-validation can also be used to compare models.

```{r}
#Errors computed and stored
cv <- cv.glm(data = alc_study, cost = loss_func, glmfit = m2, K = 10)
testerror<-cv$delta[1]
```

A ten-fold cross-validation shows that on average approximately 26% of the observations are missclassified under our model with three explanatory variables: sex, famrel and absencees. The average number of wrong predictions in the cross-validation is thus about the same as in the DataCamp exercise (26%). 

However, if the goal of a study like this would be to identify, and thereafter to implement measures to control risk factors for high alcohol usage we would not succeed very well. It seems that the model is still missing relevant risk factors. Thus, I will try to improve the model by including another variable, namely <span style="color:orange">age</span>, in the model.

```{r}
mbetter<-glm(high_use~famrel+absences+sex+age,family=binomial,data=alc)
#Errors computed and stored
cvbetter <- cv.glm(data = alc, cost = loss_func, glmfit = mbetter, K = 10)
cvbetter$delta[1] # Print the average number of wrong predictions.
```

Adding <span style="color:orange">age</span> to the model hardly improves the performance yielding just a little bit lower testing error.

### Further cross validation and model comparisons

#### Model with 12 variables

A data set with 11 integer and one factorial variable (gender) is used to improve the predictive accuracy.

```{r}
alcs<-alc[c("age","Medu","Fedu","failures","G3","sex","absences","famrel","health","goout","studytime","freetime","high_use")]
full<-glm(high_use~.,family=binomial,data=alcs)
#Errors computed and stored
cvfull <- cv.glm(data = alcs, cost = loss_func, glmfit = full, K = 10)
cvfull$delta[1] # Print the average number of wrong predictions.
```

The full model based on this data set still has a reasonably high prediction error.

#### Model reduction

I choose not to do compare model performance solely based on the number of variables, but rather based on the selection methods. To make a purposeful selection and model comparisons firstly stepAIC() and thereafter bestglm() are used. More information about the selection methods can be found in an article [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4842399/).

```{r}
library(leaps)
library(MASS)
library(bestglm)
scope<-stepAIC(full,scope=list(lower=~sex+absences+famrel,upper=full),trace=FALSE)
scope$anova

bestglm(alcs,IC="BIC",family=binomial)
```

Based on the outputs of the two approaches best models are fitted. The former (AIC) one includes gender, absences, family relationships, going out with friends and study time and the latter (best) gender, absences, family relationships and going out with friends.

```{r include=FALSE, cache=FALSE}
mAIC<-glm(high_use ~ sex + absences + famrel + goout + studytime,family=binomial,data=alcs)
mbest<-glm(high_use ~ sex + absences + famrel + goout ,family=binomial,data=alcs)
#Errors computed and stored
cvAIC <- cv.glm(data = alcs, cost = loss_func, glmfit = mAIC, K = 10)
cvAIC$delta[1] # Print the average number of wrong predictions.
cvbest <- cv.glm(data = alcs, cost = loss_func, glmfit = mbest, K = 10)
cvbest$delta[1] # Print the average number of wrong predictions.
```

```{r include=FALSE, cache=FALSE}
probabilities<-predict(full,type="response")
predictions<-probabilities >0.5
# Add the probabilities
alcsfull <- mutate(alcs, probability = probabilities)
# Calculate a logical high use value based on probabilites.
alcsfull <- mutate(alcsfull, prediction = probability > 0.5)
Training_error_full<-loss_func(class = alcsfull$high_use, alcsfull$probability)

probabilities<-predict(mAIC,type="response")
predictions<-probabilities >0.5
# Add the probabilities
alcsAIC <- mutate(alcs, probability = probabilities)
# Calculate a logical high use value based on probabilites.
alcsAIC <- mutate(alcsAIC, prediction = probability > 0.5)
Training_error_AIC<-loss_func(class = alcsAIC$high_use, alcsAIC$probability)

probabilities<-predict(mbest,type="response")
predictions<-probabilities >0.5
# Add the probabilities
alcsbest <- mutate(alcs, probability = probabilities)
# Calculate a logical high use value based on probabilites.
alcsbest <- mutate(alcsbest, prediction = probability > 0.5)
Training_error_best<-loss_func(class = alcsbest$high_use, alcsbest$probability)

```

```{r}
hu <- as.data.frame(prop.table(table(alcs$high_use)))
predfull <- as.data.frame(prop.table(table(alcsfull$prediction)))
predAIC <- as.data.frame(prop.table(table(alcsAIC$prediction)))
predbest <- as.data.frame(prop.table(table(alcsbest$prediction)))

ppobs <- ggplot(hu, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('Observed ') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","green"))+
 theme(
plot.title = element_text(color="green", size=14, face="bold.italic"),
axis.title.x = element_text(color="green", size=8, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

ppfull <- ggplot(predfull, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('Full model') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","orange"))+
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="orange", size=8, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

ppAIC <- ggplot(predAIC, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab('BestAIC model ') + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","darkgreen"))+
 theme(
plot.title = element_text(color="red", size=14, face="bold.italic"),
axis.title.x = element_text(color="darkgreen", size=8, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

ppbest<- ggplot(predbest, aes(Var1, Freq)) + geom_col(aes(fill = Var1)) + scale_y_continuous(limits = 0:1) + ylab('frequency') + xlab("Bestglm model") + theme(legend.position = 'none')+
scale_fill_manual(values=c("lightblue","blue"))+
 theme(
plot.title = element_text(color="blue", size=14, face="bold.italic"),
axis.title.x = element_text(color="blue", size=8, face="bold"),
axis.title.y = element_text(color="#993333", size=14, face="bold")
)

multiplot(ppobs, ppfull, ppAIC, ppbest,cols = 4)
```

```{r include=FALSE, cache=FALSE}
Test_error_full<-cvfull$delta[1] # Print the average number of wrong predictions.

Test_error_AIC<-cvAIC$delta[1] # Print the average number of wrong predictions.

Test_error_best<-cvbest$delta[1] # Print the average number of wrong predictions.

Training_error_full
Test_error_full
Training_error_AIC
Test_error_AIC
Training_error_best
Test_error_best
```

Model |Training error | Test error| 
--- | --- | --- | 
**<span style="color:orange">FULL model with 12 variables</span>**| 0.2094241 | 0.2329843| 
**<span style="color:green">AIC model with 5 variables</span>**| 0.2172775 | 0.2225131 | 
**<span style="color:blue">BEST model with 4 variables</span>**| 0.2041885 | **<span style="color:red">0.2198953</span>** | 

#### Final model with the lowest testing error

The last model uses best subsets approach aiming to find out the best fit model from all possible subset models. It has *only four* explanatory variables: sex, absences, family relationships and going out with friends. Using cross-validation it yield a test error of just below 22%, i.e. predicting almost 78% right.

To conclude, the results of the final model as odds ratios with confidence intervals:

```{r include=FALSE, cache=FALSE}
OR.vector <- exp(mbest$coef)
CI.vector <- exp(confint(mbest))
p.values <- summary(mbest)$coefficients[, 4]
```

```{r, results='asis'}
stargazer(mbest, coef = list(OR.vector), ci = T, 
          ci.custom = list(CI.vector), p = list(p.values), 
          single.row = T, type = "html",
          title="Variables related to alcohol consumption",
          covariate.labels=c("Gender","School absences","Family relationships","Going out with friends"),
          dep.var.caption = "Model with the lowest testing error",
          dep.var.labels = "Dependent variable:More than average alcohol usage" )

```



Ref. Fabio Pagnotta's and Hossain Mohammad Amran's Using Data Mining To Predict Secondary School Student Alcohol Consumption (2008), published by Department of Computer Science of the University of Camerino
