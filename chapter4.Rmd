
---
title: ""
output:
  html_document:
    toc: true
    toc_depth: 4
    fig_width: 10
    fig_height: 8

---

## **4<sup>th</sup> WEEK**: Clustering

Cluster analysis is one of the main tasks of exploratory data mining and is thus the topic of this week's exercise. Clustering techniques identify similar groups or clusters of observations so that members within any segment are *more similar* while data across segments are *different*. However, defining what is meant by that requires a lot of contextual knowledge and creativity.




```{r include=FALSE, cache=FALSE}

# Define packages required by this script.
library(dplyr)
library(car)
library(ggplot2)
library(stargazer)
library(GGally)
library(tidyverse)
library(corrplot)
library(MASS)
library(knitr)
library(kableExtra)
library(tableone)

#Multiplot
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

### Introduction

**Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here. (0-1 points)**

The **Boston** data will be used. It can be loaded from the R package MASS.According to the **<span style="background:lightgreen">?Boston</span>**, the data frame has 506 rows (observations) of 14 columns (variables). Briefly, the data report several variables potentially explaining housing values around Boston. Our aim is to classify suburbs from Boston data set into classes based on their characteristics. The variables included are:

- **<span style="background:lightgreen">crim</span>**: per capita crime rate by town

- **<span style="background:lightgreen">zn</span>**:proportion of residential land zoned for lots over 25,000 sq.ft.

- **<span style="background:lightgreen">indus</span>**:proportion of non-retail business acres per town.

- **<span style="background:lightgreen">chas</span>**:Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

- **<span style="background:lightgreen">nox</span>**:nitrogen oxides concentration (parts per 10 million).

- **<span style="background:lightgreen">rm</span>**:average number of rooms per dwelling.

- **<span style="background:lightgreen">age</span>**:proportion of owner-occupied units built prior to 1940.

- **<span style="background:lightgreen">dis</span>**:weighted mean of distances to five Boston employment centres.

- **<span style="background:lightgreen">rad</span>**:index of accessibility to radial highways.

- **<span style="background:lightgreen">tax</span>**:full-value property-tax rate per \$10,000.

- **<span style="background:lightgreen">ptratio</span>**:pupil-teacher ratio by town.

- **<span style="background:lightgreen">black</span>**:1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

- **<span style="background:lightgreen">lstat</span>**:lower status of the population (percent).

- **<span style="background:lightgreen">medv</span>**:median value of owner-occupied homes in \$1000s



Firstly, the data are loaded, glimpsed and thereafter summaries are printed.

```{r}
# load data
data("Boston")
glimpse(Boston)
```
```{r include=FALSE, cache=FALSE}
#one way of printing the summaries
kable(summary(Boston[,1:7]))
kable(summary(Boston[,8:14])) 
```



```{r}
tab1<-CreateTableOne(vars=c("crim","zn", "indus","chas","nox" ,"rm"     
 ,"age","dis" ,"rad" , "tax" ,"ptratio", "black"  
,"lstat" , "medv"), factorVars = c("rad", "chas"),data=Boston)
summary(tab1)
```

To get a better idea of the variables and their distributions some plots are generated.

```{r}
#density plots for numerical variables
Boston %>%
  keep(is.numeric) %>%                     # keep only numeric columns
  gather() %>%                             # convert to key-value pairs
  ggplot(aes(value)) +                     # plot the values
    facet_wrap(~ key, scales = "free") +   # in separate panels
    geom_density()                         # as density

```

```{r}
#histograms for integer variables
Boston %>%
  keep(is.integer) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(bins=10)

```


Due to variable characteristics (percents, proportions or function based values) it is understandable that some of them have rather uneven/ skewed distributions: e.g. proportion of black people (scaled proportion of blacks), indus (proportion of non-retail business acres), age (proportion of owner-occupied units built prior to 1940), proportion of land zond for very large lots (zn) and lstat (lower status of the population (percent)). 
On the contrary, dwelling size referring to the number of rooms (rm) is normally distributed and median value of owner-occupied homes can also be judged to be.
Charles River dummy variable (Chas) is binary (1/0) referring to the river crossing the area and the radial highways accessibility (rad) is an interval scaled index.

```{r}
#to get a better idea about variable crim it is plotted separately
plot(Boston$crim, col="red",pch=8, main="Crime rate per capita")
text(190,55," Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.01    0.08    0.26    3.61    3.68   88.98" )
```

We are especially interested in the variable *crime*. Crime rate varies a lot between areas ranging from min=0.01 to max=88.98. Quite a few high outlier values among the 506 observations as can be seen in the plot contribute to the low  average value of 3.61 and median value of 0.26.

To explore the relations between the variables of the data set pairwise scatter plots and a correlation plots are printed.

```{r}
#scatterplots
pairs(Boston)
```


To me the scatter plots are not that informative. Thus, I will try another approach where the correlation chart presents simultanously both the direction (color) and the magnitude (size of the circle) as well as the values of the correlation. 

```{r}
#a more visual correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
corrplot.mixed(cor_matrix,number.cex=0.65,tl.cex=0.6)
cb <- as.data.frame(cor(Boston))
 
```



```{r, results='asis'}
library(xtable)  
corstars <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                     result=c("none", "html", "latex")){
    #Compute correlation matrix
    require(Hmisc)
    x <- as.matrix(x)
    correlation_matrix<-rcorr(x, type=method[1])
    R <- correlation_matrix$r # Matrix of correlation coeficients
    p <- correlation_matrix$P # Matrix of p-value 
    
    ## Define notions for significance levels; spacing is important.
    mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
    
    ## trunctuate the correlation matrix to two decimal
    R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
    
    ## build a new matrix that includes the correlations with their apropriate stars
    Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
    diag(Rnew) <- paste(diag(R), " ", sep="")
    rownames(Rnew) <- colnames(x)
    colnames(Rnew) <- paste(colnames(x), "", sep="")
    
    ## remove upper triangle of correlation matrix
    if(removeTriangle[1]=="upper"){
      Rnew <- as.matrix(Rnew)
      Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove lower triangle of correlation matrix
    else if(removeTriangle[1]=="lower"){
      Rnew <- as.matrix(Rnew)
      Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove last column and return the correlation matrix
    Rnew <- cbind(Rnew[1:length(Rnew)-1])
    if (result[1]=="none") return(Rnew)
    else{
      if(result[1]=="html") print(xtable(Rnew), type="html")
      else print(xtable(Rnew), type="latex") 
    }
} 
corstars(Boston,result="html")
```

Finally, I create just a conservative table of correlations with notions for significance levels with the help of [this](http://www.sthda.com/english/wiki/elegant-correlation-table-using-xtable-r-package). It can be captured that there are several relevant correlations between the variables. There are strong negative correlations between weighted mean distances to five Boston employment centres (dis) and proportion of non-retail business acres per town (indus) / nitrogen oxide (nox) / and older properties (age). There is also a strong negative correlation between lower status of the population (percent) and median value of owner-occupied homes in \$1000s (medv). 
There are strong positive correlations especially between index of accessibility to radial highways (rad) and full-value property-tax rate per \$10,000 (tax) / proportion of non-retail business acres per town (indus). Indus is further positively correlated with nitrogen oxide (nox) and full-value property tax-rate (tax). 
Furthermore, the crime rate is correlated with many of the variables.It is negatively correlated with e.g. housing values and distances to employment centers, and positively correlated with e.g. access to radian highways  rad and property tax rate tax.


### Scaling the dataset and categorising crime rate

Linear discriminant analysis is a method to find linear combinations to charachterize variable classes. Firstly, the data set needs to be standardized, i.e. all variables fit to normal distribution so that the mean of every variable is zero. Scaling means subtracting the column means from the corresponding columns and dividing the difference with standard deviation:

$$ scaled(x) = \frac{x-means(x)}{sd(x)} $$


```{r}
#scale the dataset
boston_scaled <- as.data.frame(scale(Boston))
```

```{r, results='asis'}
#print out the summaries of the scaled data
#a nicer way to my opinion to print out the summary statistics
stargazer(Boston,type="html", title="Boston data (MASS package) summary statistics")
stargazer(boston_scaled,type="html", title="Boston data summary statistics AFTER SCALING")
```


In comparison to the previous summary, the values have decreased and all mean values are converted to zero. 

In addition to scaling, a categorical variable of the scaled crime rate has to be created. Quantiles are used for this to yield four grouping values: low, medium low, medium high and high crime rates. 

Next, the data set is randomly spit for the analysis to train (80%) and test (20%) sets. Thus, the train set has 404 and the test set 102 variables.


```{r}
# create a quantile vector of crim, and use it to categorize crim
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c('low','med_low','med_high','high'))
# replace the original unscaled variable.
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
# explore the categorised variable.
table(boston_scaled$crim)
```

To compare the original values by the created groups a summary is created stratified by the different crime rate categories.

```{r}
Boston2<-Boston
Boston2$crime<-boston_scaled$crim
CreateTableOne(vars=c("zn", "indus","chas","nox" ,"rm"     
 ,"age","dis" ,"rad" , "tax" ,"ptratio", "black"  
,"lstat" , "medv"), strata=c("crime"), test=FALSE, data=Boston2)
```

### Fitting the Model

#### LDA analysis on the train set


Linear Discriminant Analysis (LDA) model is carried out to classify the suburbs using the categorized crime rate as the target variable. Firstly, classification is performed on the training dataset, and thereafter the classes are predicted on the test data. 


```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
```


#### LDA (bi)plots

Based on the analysis results are plotted firstly plain, and thereafter as a LDA (bi)plot with the help of a specificifally generated "arrow"-function to add arrows. It has to be kept in mind that for plotting the classes have to tranformed from categorical to numeric.



```{r }
#helper function for the biplot arrows.
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.2, color = "deeppink", tex = 1, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
#crime classes to numeric for plotting
classes <- as.numeric(train$crime) 
#plotting the lda
plot(lda.fit, dimen = 2, col = classes, pch = classes)
```


```{r}
#(bi)plot
plot(lda.fit, dimen = 2, col = classes, pch = classes) 
#arrows 
lda.arrows(lda.fit) 
```


```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class) %>% addmargins()
```


#### Prediction

By crosstabulating the correct and predicted classes it can be seen, as expected based on the plots, that most of the ones predicted to have high crime rates indeed do. However, there are more medium low predictions than there should be. However the low areas are not recognized that well, and both the medium low and medium high classes seem to be problematic. Again, by reflecting the results to the graph it can be captured that the separation is clearest with regard to the highest class. 


### K-means

In comparison to LDA, K-means is a clustering method that divides observations into clusters. 

#### Euclidean and Manhattan distances

For K means clustering, the Boston dataset is rescaled, so that the distances are comparable. To examine the distance properties of the data and compare methods superficially the Euclidian (geometric) and Manhattan (along the axes) distance summaries are printed. 



```{r}
data(Boston)
#center and standardize variables and make it a data frame
boston_scaled<-as.data.frame(scale(Boston))
#Euclidean distance matrix
dist_eu<-dist(boston_scaled)
#for comparison Manhattan distance matrix
dist_man<-dist(boston_scaled,method = "manhattan" )
#summaries 
summary(dist_eu)
summary(dist_man)
```



#### Preliminary K-means and determination of the optimal number of clusters

K-means algorith is exploratorily ran on the dataset using 5 clusters.


```{r }
#kmeans using euclidean and five clusters
km <- kmeans(dist_eu, centers = 5)
pairs(boston_scaled, col = km$cluster, lower.panel = NULL)
```


At the first sight this looks a little like fireworks. Before interpreting the plot in more detail I will estimate the optimal number of clusters using the total within cluster sum of squares (WCSS) with the number of clusters ranging from 1 to 10 and visualize the results.


```{r}
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
#Plot the results
plot(1:k_max, twcss, type = 'b')
```



The most prominent change happens at 2 refering to choosing 2 as the most appropriate number of clusters. However, also the drop between 2 and 3 could potentially be interpreted as a radical one. Yet, I choose to carry out the k-means clustering again using 2 as the number of clusters.

#### K- means using the optimal number of clusters

```{r }
km <- kmeans(dist_eu, centers = 2)
pairs(boston_scaled, col = km$cluster, lower.panel = NULL)
```

The new pairwise scatter plot with only two clusters looks better than the previous one. However, it is still, to my opinion somewhat poor with regard to clarity or easiness to interpret. All data points are assigned to two different clusters with different colours. The variables for which the colours  seem clearly separated are more relevant for clustering than for those they seem very mixed. It can be seen that some of the variables, but not all, are indeed clearly divided between the clusters.For example points with tax and rad values belong to the red cluster, while points with high values belong to the black cluster.  

For example points with low rad (index of accessibility to radial highways) values in general belong to the red cluster, and points with high rad values belong to the black cluster. 

Thus rad seems useful for clustering purposes. Another relevant variable is e.g. tax (full-value property-tax rate). However, for example the values of the variable chas don't seem to give any indication about which cluster the points belong to, and thus chas is not very relevant for clustering. Thus this visualisation is helpful in showing whether the variables or variable pairs are good indicators about which cluster a specific data point belongs to.


 Crime rate clearly (with a few exceptions) belongs to one cluster seen in red and proportion of residential owned lots (zn) to other cluster coloured black. To the same cluster with crime rate belong most of the observations from lower status of population (lstat), proportion of blacks (black) and nitrogen oxide concentration (nox). This means that part of town with high crime rate has also most likely lower status population, black people and high amounts of nitrogen oxide in the air. Areas with high amount of residential owned plots are likely to have accessibility to radial highways (rad), high full value tax rate (tax) and more pupils towards one teacher.


### Bonus



K-means is performed on the scaled Boston data using 4 clusters. Therafter, LDA is fitted using the generated clusters as target classes. Biplot is printed.
```{r }
data("Boston")
boston_scaled <- scale(Boston)
dist_eu <- dist(Boston)
km <-kmeans(dist_eu, centers = 4)
pairs(Boston, col = km$cluster)
lda.fit2 <- lda(km$cluster ~., data = Boston)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
plot(lda.fit2, dimen = 2, col= classes, pch=classes)
lda.arrows(lda.fit2, myscale = 2)
```

Nitrogen oxides concentration (nox) is the longest arrow and it divides the data frames observations into two separate areas. From the other variables it is hard to say properly.



