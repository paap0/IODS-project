
---
title: ""
output:
  html_document:
    toc: true
    toc_depth: 4


---

## <span style="color:lightgreen">**4<sup>th</sup> WEEK**: Clustering and classification</span>

Cluster analysis is one of the main tasks of exploratory data mining and is thus the topic of this week's exercise. Clustering techniques identify similar groups or clusters among observations so that members within any segment are *more similar* while data across segments are *different*. However, defining what is meant by that requires often a lot of contextual knowledge and creativity.




```{r include=FALSE, cache=FALSE}

# Define packages required by this script.
library(dplyr)
library(car)
library(ggplot2)
library(stargazer)
library(GGally)
library(tidyverse)
library(corrplot)
library(MASS)
library(knitr)
library(kableExtra)
library(tableone)

#Multiplot
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```


### <span style="color:lightgreen">Introduction</span>



The **Boston** data will be used. It can be loaded from the R package MASS.According to the **<span style="background:lightgreen">?Boston</span>**, the data frame has 506 rows (observations) of 14 columns (variables). Briefly, the data report several variables potentially explaining housing values around Boston. Our aim is to classify the included suburbs from Boston data set into classes based on their characteristics. The variables are:


- **<span style="background:lightgreen">crim</span>**: per capita crime rate by town

- **<span style="background:lightgreen">zn</span>**:proportion of residential land zoned for lots over 25,000 sq.ft.

- **<span style="background:lightgreen">indus</span>**:proportion of non-retail business acres per town.

- **<span style="background:lightgreen">chas</span>**:Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

- **<span style="background:lightgreen">nox</span>**:nitrogen oxides concentration (parts per 10 million).

- **<span style="background:lightgreen">rm</span>**:average number of rooms per dwelling.

- **<span style="background:lightgreen">age</span>**:proportion of owner-occupied units built prior to 1940.

- **<span style="background:lightgreen">dis</span>**:weighted mean of distances to five Boston employment centres.

- **<span style="background:lightgreen">rad</span>**:index of accessibility to radial highways.

- **<span style="background:lightgreen">tax</span>**:full-value property-tax rate per \$10,000.

- **<span style="background:lightgreen">ptratio</span>**:pupil-teacher ratio by town.

- **<span style="background:lightgreen">black</span>**:1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

- **<span style="background:lightgreen">lstat</span>**:lower status of the population (percent).

- **<span style="background:lightgreen">medv</span>**:median value of owner-occupied homes in \$1000s




Firstly, the data are loaded, glimpsed and thereafter summaries are printed.

```{r message=FALSE, warning=FALSE}
# load data
data("Boston")
glimpse(Boston)
```

### <span style="color:lightgreen">Summaries and plots</span>

#### <span style="color:lightgreen">Basic variable characteristics</span>

The descriptive summary includes values of skewness (a measure of the symmetry in a distribution) and kurtosis (measuring the [*"tail-heaviness"*](https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics) of the distribution) and already shows the separating quantile values for crime to be further used later.
```{r message=FALSE, warning=FALSE}
tab1<-CreateTableOne(vars=c("crim","zn", "indus","chas","nox" ,"rm"   
 ,"age","dis" ,"rad" , "tax" ,"ptratio", "black"  
,"lstat" , "medv"), factorVars = c("rad", "chas"),data=Boston)
summary(tab1)
```


#### <span style="color:lightgreen">Distribution plots</span>

To get a better idea of the variables and their distributions some plots are generated.



```{r message=FALSE, warning=FALSE, fig.align="center"}
#density plots for numerical variables
Boston %>%
  keep(is.numeric) %>%                     # keep only numeric columns
  gather() %>%                             # convert to key-value pairs
  ggplot(aes(value)) +                     # plot the values
    facet_wrap(~ key, scales = "free") +   # in separate panels
    geom_density()                         # as density

```

```{r message=FALSE, warning=FALSE, fig.width=5,fig.height=2, fig.align="center"}
#histograms for integer variables
Boston %>%
  keep(is.integer) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(bins=10)



```




Due to variable characteristics (percents, proportions or function based values) it is understandable that most of them have rather uneven/ skewed distributions: e.g. proportion of black people (scaled proportion of blacks), indus (proportion of non-retail business acres), age (proportion of owner-occupied units built prior to 1940), proportion of land zond for very large lots (zn) and lstat (lower status of the population (percent)). 
On the contrary, dwelling size referring to the number of rooms (rm) is normally distributed and median value of owner-occupied homes (medv) can also be judged to be.
Charles River dummy variable (chas) is binary (1/0) referring to the river crossing the area and the radial highways accessibility (rad) is an interval scaled index.

#### <span style="color:lightgreen">Crime rate per capita</span>

```{r message=FALSE, warning=FALSE, fig.width=6,fig.height=5, fig.align="center"}
#to get a better idea about variable crim it is plotted separately
plot(Boston$crim, col="red",pch=8, main="Crime rate per capita")
text(198,59," Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.01    0.08    0.26    3.61    3.68   88.98" )
```
```{r message=FALSE, warning=FALSE, fig.width=6,fig.height=5, fig.align="center"}
ggplot(Boston, aes(x = crim)) +
  stat_density(aes(col="red"),position="identity",geom="line",size=2)+
  ggtitle("Crime rate per capita")+ theme(legend.position="none")

```
We are especially interested in the variable *crim*. Thus, it is visualized separately. Crime rate varies a lot between areas ranging from min=0.01 to max=88.98. Quite a few high outlier values among the 506 observations as can be seen in the plot contribute to the low  average value of 3.61 and median value of 0.26. The distribution is strongly skewed to the left.

#### <span style="color:lightgreen">Variable correlations</span>



To explore the relations between the variables of the data set pairwise scatter plots and a correlation plots are printed.

```{r message=FALSE, warning=FALSE, fig.width=8,fig.height=7, fig.align="center"}
#scatterplots
pairs(Boston,lower.panel = NULL)
```


To me these scatter plots are not that informative, though. Thus, I will try another approach where the correlation chart presents simultanously both the direction (color) and the magnitude (size of the circle) as well as the values of the correlation. 
```{r {r fig2, fig.height = 8, fig.width = 7, fig.align = "center"}
#a more visual correlation matrix
cor_matrix<-cor(Boston) %>% round(2)
corrplot.mixed(cor_matrix,number.cex=0.65,tl.cex=0.6)
```
And, finally, I create just a conservative table of correlations with notions for significance levels with the help of [this](http://www.sthda.com/english/wiki/elegant-correlation-table-using-xtable-r-package). 
```{r, results='asis', fig.align="center", message=FALSE, warning=FALSE}
library(xtable)  
corstars <-function(x, method=c("pearson", "spearman"), removeTriangle=c("upper", "lower"),
                     result=c("none", "html", "latex")){
    #Compute correlation matrix
    require(Hmisc)
    x <- as.matrix(x)
    correlation_matrix<-rcorr(x, type=method[1])
    R <- correlation_matrix$r # Matrix of correlation coeficients
    p <- correlation_matrix$P # Matrix of p-value 
    
    ## Define notions for significance levels; spacing is important.
    mystars <- ifelse(p < .0001, "****", ifelse(p < .001, "*** ", ifelse(p < .01, "**  ", ifelse(p < .05, "*   ", "    "))))
    
    ## trunctuate the correlation matrix to two decimal
    R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
    
    ## build a new matrix that includes the correlations with their apropriate stars
    Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
    diag(Rnew) <- paste(diag(R), " ", sep="")
    rownames(Rnew) <- colnames(x)
    colnames(Rnew) <- paste(colnames(x), "", sep="")
    
    ## remove upper triangle of correlation matrix
    if(removeTriangle[1]=="upper"){
      Rnew <- as.matrix(Rnew)
      Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove lower triangle of correlation matrix
    else if(removeTriangle[1]=="lower"){
      Rnew <- as.matrix(Rnew)
      Rnew[lower.tri(Rnew, diag = TRUE)] <- ""
      Rnew <- as.data.frame(Rnew)
    }
    
    ## remove last column and return the correlation matrix
    Rnew <- cbind(Rnew[1:length(Rnew)-1])
    if (result[1]=="none") return(Rnew)
    else{
      if(result[1]=="html") print(xtable(Rnew), type="html")
      else print(xtable(Rnew), type="latex") 
    }
} 
# Make table with centered columns
corstars(Boston,result="html")

```


&nbsp;

From all the information above it can be captured that there are several relevant correlations between the variables. Strong negative correlations exist between weighted mean distances to five Boston employment centres (dis) and proportion of non-retail business acres per town (indus) / nitrogen oxide (nox) / and older properties (age). Not surprisingly, a strong negative correlation between lower status of the population (percent) and median home value (medv) is seen.
Strong positive correlations especially between index of accessibility to radial highways (rad) and full-value property-tax rate per \$10,000 (tax) / proportion of non-retail business acres per town (indus) exist. Proportion of non-retail business acres per town (indus) is further positively correlated with nitrogen oxide (nox) and full-value property tax-rate (tax). 
Furthermore, one of our main interests, the crime rate is correlated with many of the variables: e.g. negatively with e.g. distance to employment centers (dis) and median home value (medv), positively with e.g. full-value property tax-rate (tax) and access to radial highways (rad). Thus, an increase in crime rate seems to be associated with an increasing highway accessibility index and property tax.


### <span style="color:lightgreen">Scaling the dataset and categorising crime rate</span>

Linear discriminant analysis is a method generating linear combinations to charachterize variable classes. To enable the method the data set needs to be standardized, i.e. all variables fit to normal distribution so that the mean of every variable is zero by ubtracting the column means from the corresponding columns and dividing the difference with standard deviation:

<span style="color:lightgreen">$$ scaled(x) = \frac{x-means(x)}{sd(x)} $$</span>


```{r message=FALSE, warning=FALSE}
#scale the dataset
boston_scaled <- as.data.frame(scale(Boston))
```


```{r setup, include=FALSE, fig.align="center"}
#print out the summaries of the scaled data
#a nicer way to my opinion to print out the summary statistics
stargazer(Boston,type="html", title="Boston data (MASS package) summary statistics")
stargazer(boston_scaled,type="html", title="Boston data summary statistics AFTER SCALING")
```




In comparison, the values of the latter table have decreased, and all mean values are converted to zero and standard deviations to 1.

In addition to scaling, a categorical variable of the scaled crime rate has to be created. Quantiles are used for this to yield four grouping values: *low, medium low, medium high* and *high crime* rates and thus four groups with approximatey equal numbers of observation each.


Next, the data set is randomly spit for the analysis to train (80%) and test (20%) sets. Thus, the train set has 404 and the test set 102 variables.


```{r message=FALSE, warning=FALSE}
# create a quantile vector of crim, and use it to categorize crim
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c('low','med_low','med_high','high'))
# replace the original unscaled variable.
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
# explore the categorised variable.
table(boston_scaled$crim)
```


To compare how the original values by the created groups differ between the groups a summary is created stratified by the different crime rate categories. There are many large differences, especially when comparing the high and low crime rate groups.
```{r fig.align="center", warning=FALSE,message=FALSE}
Boston2<-Boston
Boston2$crime<-boston_scaled$crim
CreateTableOne(vars=c("zn", "indus","chas","nox" ,"rm"     
 ,"age","dis" ,"rad" , "tax" ,"ptratio", "black"  
,"lstat" , "medv"), strata=c("crime"), test=FALSE, data=Boston2)
```



### <span style="color:lightgreen">Fitting the model</span>



#### <span style="color:lightgreen">LDA analysis on the train set</span>


Linear Discriminant Analysis (LDA) model is carried out to classify the suburbs using the categorized crime rate as the target variable. Firstly, classification is performed on the training dataset, and thereafter the classes are predicted on the test data. 


```{r message=FALSE, warning=FALSE}
set.seed(123)
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
```


#### <span style="color:lightgreen">LDA (bi)plots</span>


Based on the analysis results are plotted firstly plain, and thereafter as a LDA (bi)plot with the help of a specificifally generated "arrow"-function to add arrows. It has to be kept in mind that for plotting the classes have to tranformed from categorical to numeric.

```{r p1,fig.height = 5, fig.width = 6, fig.align = "center", message=FALSE, warning=FALSE}
#helper function for the biplot arrows.
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.2, color = "deeppink", tex = 1, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
#crime classes to numeric for plotting
classes <- as.numeric(train$crime) 
#plotting the lda
p1<-plot(lda.fit, dimen = 2, col = classes, pch = classes)
```


```{r p2,fig.height = 5, fig.width = 6, fig.align = "center", message=FALSE, warning=FALSE}
#(bi)plot
p2<-plot(lda.fit, dimen = 2, col = classes, pch = classes)
#arrows 
lda.arrows(lda.fit) 
```
```{r fig.height = 5, fig.width = 6, fig.align = "center", message=FALSE, warning=FALSE}
print(lda.fit) 
```

#### <span style="color:lightgreen">Prediction</span>

```{r message=FALSE, warning=FALSE}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class) %>% addmargins()
```


By crosstabulating the correct and predicted classes it can be seen that the model's predictions are quite accurate with the high category in the test data. On the contrary, the low areas are not recognized that well, and both the medium low and medium high classes seem to be problematic. By reflecting the results to the graph it can be captured that the separation is clearest with regard to the highest class. Due to that the prediction accuracies are understandable. 


### <span style="color:lightgreen">K-means</span>


In comparison to LDA, K-means is a clustering method that divides observations into clusters.

#### <span style="color:lightgreen">Eclidean and Manhattan distances</span>


For K means clustering, the Boston dataset is rescaled, so that the distances are comparable. To examine the distance properties of the data and compare methods superficially both the Euclidian (geometric) and Manhattan (along the axes) distance summaries are printed. 



```{r message=FALSE, warning=FALSE}
data(Boston)
#center and standardize variables and make it a data frame
boston_scaled<-as.data.frame(scale(Boston))
#Euclidean distance matrix
dist_eu<-dist(boston_scaled)
#for comparison Manhattan distance matrix
dist_man<-dist(boston_scaled,method = "manhattan" )
#summaries 
summary(dist_eu)#Euclidian
summary(dist_man)#Manhattan
```




#### <span style="color:lightgreen">Preliminary K-means and determination of the optimal number of clusters</span>



K-means algorith is exploratorily ran on the dataset using 5 clusters.


```{r message=FALSE, warning=FALSE, fig.width=8,fig.height=7, fig.align="center"}
#kmeans using euclidean and five clusters
km <- kmeans(dist_eu, centers = 5)
pairs(boston_scaled, col = km$cluster,lower.panel = NULL)
```


At the first sight the plotted reslts look a little like fireworks. Before interpreting the plot in more detail the optimal number of clusters using the total within cluster sum of squares (WCSS) with the number of clusters ranging from 1 to 10 is estimated and the results visualized.
```{r message=FALSE, warning=FALSE, fig.width=4,fig.height=4,fig.align="center"}
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
#Plot the results
plot(1:k_max, twcss, type = 'b')
```



The most prominent change in the sum of squares happens at 2. However, there are still drops after two, too, but they only yield small improvements. Yet, I choose to carry out the k-means clustering again using 2 as the number of clusters.

#### <span style="color:lightgreen">K- means using the optimal number of clusters</span>


```{r message=FALSE, warning=FALSE, fig.width=8,fig.height=7, fig.align="center"}
km <- kmeans(dist_eu, centers = 2)
pairs(boston_scaled, col = km$cluster, lower.panel = NULL)
```



The new pairwise scatter plot with only two clusters looks better than the previous one.  All data points are assigned to two red/black clusters. The clearer separation for the colours the more relevant for clustering the variable. Property tax and access to radial highways seem to discriminate quite well between the two clusters.



### <span style="color:lightgreen">Bonus</span>



K-means is performed on the scaled Boston data using 4 clusters. Therafter, LDA is fitted using the generated clusters as target classes. Biplot is printed.
```{r message=FALSE, warning=FALSE, fig.width=8,fig.height=7, fig.align="center"}
set.seed(123)
data(Boston)
boston_scaled4 <- as.data.frame(scale(Boston,center=TRUE,scale = TRUE))
dist_eu4 <- dist(boston_scaled4)
km2 <-kmeans(dist_eu4, centers = 4)
#pairs(boston_scaled4, col = km$cluster, lower.panel = NULL)
```

```{r message=FALSE, warning=FALSE, fig.width=8,fig.height=5, fig.align="center"}
boston_scaled4$classes<-km2$cluster
lda.fit2 <- lda(boston_scaled4$classes ~., data = boston_scaled4)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
plot(lda.fit2, dimen = 2, col= as.numeric(boston_scaled4$classes), pch=classes)
lda.arrows(lda.fit2, myscale = 2)
```





### <span style="color:lightgreen">SUPER-Bonus</span>

Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.

```{r}
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```
Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.

```{r message=FALSE, warning=FALSE,fig.align="center"}
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```
        
```{r message=FALSE, warning=FALSE,fig.align="center"}
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train$crim)
```
```{r message=FALSE, warning=FALSE,fig.align="center"}
train$cluster<-km2$cluster[match(rownames(train),rownames(boston_scaled4))]
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=as.factor(train$cluster))
```