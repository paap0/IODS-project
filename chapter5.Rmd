---
title: ""
output:
  html_document:
    toc: true
    toc_depth: 4
---

## <span style="color:purple">**5<sup>th</sup> WEEK**: Dimensionality reduction</span>


### <span style="color:purple">Introduction</span>

[Data](http://hdr.undp.org/en/content/human-development-index-hdi) from the United Nations Development Programme are used in this exercise.They encompass e.g. longevity, health and well-being related criteria for the development of the country.
Data have been further edited according to [this script](https://github.com/paap0/IODS-project/blob/master/data/create_human.R) to finally yield a dataset of 155 countries and 8 variables. Gender inequality variables with regard to education and labour force are added, and thereafter Gender inequality- and Human development- datasets are joined by the variable country.
In addition, rows with missing values are removed and only countries are included.

Variable labels and their explanations are described below:


```{r include=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
rm(list = ls())
# Define packages required by this script.
library(dplyr)
library(car)
library(ggplot2)
library(stargazer)
library(GGally)
library(tidyverse)
library(corrplot)
library(MASS)
library(knitr)
library(kableExtra)
library(tableone)
library(dplyr)
library(knitr)
library(DT)
library(xtable)
library(factoextra)
```
Data are loaded and glimpsed.
```{r echo=FALSE, message=FALSE, warning=FALSE}
# load data
human<-read.csv(file="human5.csv", row.names=1,header=TRUE)
glimpse(human)
```

### <span style="color:purple">Summary and graphical overview</span>

```{r echo=FALSE, fig.height=7, fig.width=7, message=FALSE, warning=FALSE,fig.align="center"}
Label<-c("edu2.f_m" ,     "lab.f_m",       "edu.exp"    ,   "life.exp",
 "GNI"     ,      "mat.mort"  ,    "ad.birth.rate", "parl.prop"  )

Variable<-c("Population with at least some secondary education (female/male ratio)",
 "Labour force participation rate (female/male ratio)",
"Expected years of schooling",
"Life expectancy at birth",
"Gross national income (GNI) per capita",
"Maternal mortality ratio(deaths per 100 000 live births)",
"Adolescent birth rate (births per 1 000 women ages 15-19)",
"Share of seats in parliament (female)")

om<- data.frame(Label,Variable)

kable(om, "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Next, detailed summary statistics and box plots are printed.

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE,fig.align="center"}
library(settings)
reset(options)
options("scipen"=10, "digits"=2)
tab1<-CreateTableOne(vars=c( "edu2.f_m" ,"lab.f_m" ,  "edu.exp"   ,   
"life.exp" ,     "GNI"  ,         "mat.mort"  ,    "ad.birth.rate",
"parl.prop") ,data=human)
summary(tab1)
reset(options)
```


```{r echo=FALSE, message=FALSE, warning=FALSE,fig.width=10,fig.align="center"}
par(mfrow=c(2,4))
p1<-boxplot(human$edu2.f_m,col="lightblue",main="F/M educational ratio")
#print("Max and min value countries for educational levelF/M ratio")
#rbind(human[which(human$edu2.f_m==max(human$edu2.f_m)),],
#human[which(human$edu2.f_m==min(human$edu2.f_m)),])
p2<-boxplot(human$lab.f_m,col="brown",main="F/M labour force ratio")
#print("Max and min value countries for labour force F/M ratio")
#rbind(human[which(human$lab.f_m==max(human$lab.f_m)),],
#human[which(human$lab.f_m==min(human$lab.f_m)),])
p3<-boxplot(human$edu.exp,col="blue",main="Expected years of schooling")
#print("Max and min value countries for schooling years")
#rbind(human[which(human$edu.exp==max(human$edu.exp)),],
#human[which(human$edu.exp==min(human$edu.exp)),])
p4<-boxplot(human$life.exp,col="green",main="Life expectancy")
#print("Max and min value countries for life expectancy")
#rbind(human[which(human$life.exp==max(human$life.exp)),],
#human[which(human$life.exp==min(human$life.exp)),])
p5<-boxplot(human$GNI,col="red",main="General income")
#print("Max and min value countries for general income")
#rbind(human[which(human$GNI==max(human$GNI)),],
#human[which(human$GNI==min(human$GNI)),])
p6<-boxplot(human$mat.mort,col="grey",main="Maternal mortality (per 100000)")
#print("Max and min value countries for maternal mortality")

#rbind(human[which(human$mat.mort==max(human$mat.mort)),],
#human[which(human$mat.mort==min(human$mat.mort)),])
p7<-boxplot(human$ad.birth.rate,col="darkred",main="Adolescent birth rate")
#print("Max and min value countries for adolescent birth rate")

#rbind(human[which(human$ad.birth.rate==max(human$ad.birth.rate)),],
#human[which(human$ad.birth.rate==min(human$ad.birth.rate)),])
p8<-boxplot(human$ad.birth.rate,col="pink",main="Share of female seats in parliament")
#print("Max and min value countries for female share in parliament")
#rbind(human[which(human$parl.prop==max(human$parl.prop)),],
#human[which(human$parl.prop==min(human$parl.prop)),])


#rownames(rbind(human[which(human$parl.prop==max(human$parl.prop)),],
#human[which(human$parl.prop==min(human$parl.prop)),]))
```

### <span style="color:purple">Values from Finland with extreme value countries</span>


Many of the variables have quite a few outliers meaning that the countries differ tremendeously with each other - as expected. For my own interest, and overall comparison of the countries with extreme values (max and min value countries of each variable according to the order of them) an additional table with an additional row with our own national values is printed.
```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE,fig.align="center"}

#print("Max and min value countries for educational levelF/M ratio")
a<-rbind(human[which(human$edu2.f_m==max(human$edu2.f_m)),],
human[which(human$edu2.f_m==min(human$edu2.f_m)),])

#print("Max and min value countries for labour force F/M ratio")
b<-rbind(human[which(human$lab.f_m==max(human$lab.f_m)),],
human[which(human$lab.f_m==min(human$lab.f_m)),])

#print("Max and min value countries for schooling years")
c<-rbind(human[which(human$edu.exp==max(human$edu.exp)),],
human[which(human$edu.exp==min(human$edu.exp)),])

#print("Max and min value countries for life expectancy")
d<-rbind(human[which(human$life.exp==max(human$life.exp)),],
human[which(human$life.exp==min(human$life.exp)),])

#print("Max and min value countries for general income")
e<-rbind(human[which(human$GNI==max(human$GNI)),],
human[which(human$GNI==min(human$GNI)),])

#print("Max and min value countries for maternal mortality")

f<-rbind(human[which(human$mat.mort==max(human$mat.mort)),],
human[which(human$mat.mort==min(human$mat.mort)),])

#print("Max and min value countries for adolescent birth rate")

g<-rbind(human[which(human$ad.birth.rate==max(human$ad.birth.rate)),],
human[which(human$ad.birth.rate==min(human$ad.birth.rate)),])

#print("Max and min value countries for female share in parliament")

h<-rbind(human[which(human$parl.prop==max(human$parl.prop)),],
human[which(human$parl.prop==min(human$parl.prop)),])

out<-rbind(a,b,c,d,e,f,g,h,human["Finland",])

kable(out, "html") %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

The variables ranges are indeed large. Life exectancy is highest, not surprisingly, in Japan with the age of 84 and lowest in Swaziland being only 49 years. Extreme maternal mortality ratio is reported in Sierra Leone and adolescent birth rate in Niger. A variable related to wealth, namely general income, is highest in Qatar with the value of 123 124,
and a huge difference exists to the poorest country, Central African Republic reporting a value of 581. Surprisingly, the female / male ratio for population with at least secondary school eduction is highest in Myanmar.

### <span style="color:purple">Variable correlations</span>

```{r echo=FALSE, fig.height=10, fig.width=10, message=FALSE, warning=FALSE, paged.print=FALSE,fig.align="center"}
cor_fun <- function(data, mapping, method="pearson", ndp=2, sz=5, stars=TRUE, ...){

    data <- na.omit(data[,c(as.character(mapping$x), as.character(mapping$y))])

    x <- data[,as.character(mapping$x)]
    y <- data[,as.character(mapping$y)]

    corr <- cor.test(x, y, method=method)
    est <- corr$estimate
    lb.size <- sz* abs(est) 

    if(stars){
      stars <- c("***", "**", "*", "")[findInterval(corr$p.value, c(0, 0.001, 0.01, 0.05, 1))]
      lbl <- paste0(round(est, ndp), stars)
    }else{
      lbl <- round(est, ndp)
    }

    ggplot(data=data, mapping=mapping) + 
      annotate("text", x=mean(x), y=mean(y), label=lbl, size=lb.size,...)+
      theme(panel.grid = element_blank())
  }


ggpairs(human%>%mutate_all(as.numeric), 
        lower=list(continuous=wrap("smooth", colour="purple")),
        diag=list(continuous=wrap("barDiag", fill="purple")),
        upper=list(continuous=cor_fun),title="Graphical overview of the 8 variables")

```
&nbsp;

The only variable that seems normally distributed is expected years of shooling.
From the variable connectivity, it can be captured that there are several relevant correlations apart from labour force participation ratio and female share in parliament. Not unexpectedly, life expectancy is negatively correlated with maternal mortality and adolescent birth rate. On the contrary, it is positively correlated with expected years of education and general income.  Expected years of education also has negative correlations, e.g. with maternal mortality. 
To encompass, it seems basically that the more educated women there is, the better the longevity, the higher the overall educational expectancy and general income and lower maternal mortality and adolescent birth rate, and, interestingly, two variables (lab.f_m and parl.prop) are only weakly correlated with any of the other variables.


### <span style="color:purple">Principal component analysis</span>



To begin with, it has to clarified, that PCA is an unsupervised approach. This means that the directions of the generated components are identified without using a response variable (Y) to determine their direction. In other words PCA focuses on recognizing sets of characteristics without an association to any response variable.PCA extracts important variables in form of coponents from a large set of variables available in a dataset.The main aim to recognize relationships between these charasteristics, thus, extract low dimensional set of features from a high dimensional dataset with to capture as much information as possible. With fewer variables further visualization of the data is more meaningful, but the method can also be used to editing data for subsequent analyses. Components are expressed by relationships of the original variables, they do not correlate with each other and each is less important than the previous one in terms of explained variance.

#### <span style="color:purple">PCA on unscaled data</span>

According to the instructions, PCA will be run twice (with unscaled and scaled predictors). Firstly, the former analysis is carried out, and a biplot is created.
Biplots are basically scatter plots using observations as x and 2 principal components as y coordinates. Labeled arrows connect original variables to the principal components and their length are proportional to the standard deviations. Small angle between a variable and a PC axis reffers to high positive correlation.

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE,fig.align="center"}
pca_human <- prcomp(human)
summary(pca_human)
biplot(pca_human, choices = 1:2, cex=c(0.9, 1), col = c("lightgray", "purple"))
```

As shown in image below, first principal component is dominated by one variable only: GNI. This is due to high value of variance associated with the variable. 

When the variables are scaled, we get a much better representation of variables in 2D space. Thus, the matrix used should be numeric and have standardized data.

Next, the data are scaled and PCA is ran again.


#### <span style="color:purple">PCA on scaled data</span>



```{r echo=FALSE, fig.align="center", fig.height=8, fig.width=8, message=FALSE, warning=FALSE, fi.width=8,fig.align="center"}
human_std <- scale(human)
pca_human_std <- prcomp(human_std)
biplot(pca_human_std, choices = 1:2, cex=c(0.6, 1), col = c("lightgrey", "purple"))
```


It can be seen that scaling dramatically changes results, because the different scaling factors directly influence calculation of the PC components. GNI does not dominate the variance anymore, and the results can be interpreted: 
First principal component is a linear combination of original variables and captures the maximum variance in the dataset. It determines the direction of highest variability in the dataset. Further, the first principal component results in a line which is closest to the data minimizing the sum of squared distance between a data point and the line. The generated PC1 covers 53.6 percent of the variation here. Characteristics of the PC1 are high maternal mortality ratio and adolescent birth rate (positive loadings) as well as expected schooling years, life expectancy, female educational proportion and income (negative loadings). This component thus captures longevity and educational aspects. By looking at the graph, it can be further interpreted that when maternal mortality and adolescent birth rate are low education, longevity, women schooling and general income are high and vice versa.
The second component is also a linear combination of original predictors and it aims to capture *the remaining variance* in the data set and is *uncorrelated* with the first component (the correlation between first and second component should is zero). In this example, PC2 covers 16.2 percent of the variation. PC2 encompasses labour force participation ratio and female share in parliament, which were already recognized as "different kind of"-variables at the first preliminary investigation of the data. It could be scrutinized as "gender equality"-component.



### <span style="color:purple">Multiple correscondence analysis</span>


Corresponcence or multiple correspondence analysis can be used in dimensionality reduction in cases of categorical variables. MCA is a generalization of PCA and an extension of CA. Basically, cross-tabulations are used to provide input for graphically present the data. Methods can be used for visualization or pre-editing of the data.

#### <span style="color:purple">Tea data and My tea data</span>

We practice multiple corresponce analysis using the tea dataset and MCA() function that come in the package "FactoMineR" by Francois Husson, Julie Josse, Sebastien Le, and Jeremy Mazet. Additionally, I use "factoextra".

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.align="center"}
library(FactoMineR)
data(tea)
str(tea)
```

FactoMineR package is required with its tea dataset reporting a questionnare on tea drinking habits. From the collected 18 variables I choose to use altogether six categorical variables: where, work, How, how, age_Q and sex. To both see the categories and number (percentage) of respondents in each of them a summary is printed. Based on my interest I stratify it by gender:

#### <span style="color:purple">My tea data summary and graphical overview</span>

```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, warnings=FALSE,fig.align="center"}
keep <- c("where", "work","How", "how","age_Q", "sex")
dftea<- dplyr::select(tea, one_of(keep))
CreateTableOne(vars=keep,strata=c("sex"),factorVars=keep,data=dftea )
```

Graphical overview: 
```{r echo=FALSE, fig.width=10, message=FALSE, warning=FALSE, warnings=FALSE,fig.align="center"}

gather(dftea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar(fill="purple") + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))+   scale_fill_manual("key")
```

There seems to be some variation in each of the variables. The category with the lowest frequency seems to be in the "How"-variable (other; only 9 observations), however, not too low to potentially distort MCA analysis. 
Age distribution shows, that the young ones are the largest group having answered the questionnaire and among them females are overrepresented. On the other hand, in the group of 25-34 year olds there are a lot more males than there are females. Altogether, there are more female observations than there are males. However, there are respondents in each age group. Surprisingly, as I assume this dataset to originate from Great Brittain, most respondents have reported to use either tea bags or tea bags and unpacked tea, both. A minority uses unpackaged tea, which I would have thought to be "the right Brittish manner". Most drink their tea alone, some use milk, a few lemon. There is no honey option at all, and sugar variable is recorded as a separate variable and is not used in this analysis.  Most buy their tea in a chain store, and men visit tea shops more often than women - again surprising phenomena to me. And finally, tea is mostly drank outside work.

#### <span style="color:purple">MCA</span>

Next MCA analysis on the chosen tea data is carried out. To do that, firstly, a crosstabulated frequency table is standardized to yield relative frequencies across the cells to sum up to 1.0. The aim of a MCA analysis is to represent the entries in the table of relative frequencies in terms of the distances between individual rows and/or columns in a low-dimensional space.

The output of the MCA() function is a list including :

```{r echo=FALSE, message=FALSE, warning=FALSE, warnings=FALSE,fig.align="center"}
tea_mca <- MCA(dftea, graph = FALSE)
print(tea_mca)
```

and looks like this for the chosen tea-dataset:
```{r echo=FALSE, message=FALSE, warning=FALSE, warnings=FALSE,fig.align="center"}
summary(tea_mca)
```

The dimdesc function might help to interpret the dimensions. It allows to see which variables the axes are the most linked to, i.e. which categories describe the best each axis.  

```{r echo=FALSE, message=FALSE, warning=FALSE, warnings=FALSE,fig.align="center"}
dimdesc(tea_mca,axes=1:2,proba=0.05)
```

##### <span style="color:purple">Scree plot</span>

To visualize the percentage of inertia explained by each MCA dimensions:
```{r echo=FALSE, message=FALSE, warning=FALSE,fig.align="center"}
fviz_screeplot(tea_mca, addlabels = TRUE, ylim = c(0, 22))
```

The first two dimensions of MCA explain *only* about 26% of variance. Thus, already at this point I think I could perhaps have chosen my variables better to explain each others variation more.

##### <span style="color:purple">Biplots</span>

To further clarify the MCA results graphical representation is used. Firstly, there is biplot showing the global pattern within the data. Observations are represented by blue points and variables by red triangles and labels. The distance between any observation points or variable points gives a measure of their similarity (or dissimilarity). Similar types of individuals are close on the map, as well as similar kinds of variables.

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE,fig.align="center"}
fviz_mca_biplot(tea_mca, 
               repel = TRUE, # Avoid text overlapping (slow if many point)
               ggtheme = theme_minimal())
```

Next, a plot is created to visualize the correlation between variables and MCA principal dimensions:


```{r echo=FALSE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE, fig.align="center"}
fviz_mca_var(tea_mca, choice = "mca.cor", 
            repel = TRUE, # Avoid text overlapping (slow)
            ggtheme = theme_minimal())
```

The plot should help to identify variables that are the most correlated with each dimension. The squared correlations between variables and the dimensions are used as coordinates.


And finally, as is described in the exercise instructions: "The typical graphs show the original classes of the discrete variables on the same "map", making it possible to reveal connections (correspondences) between different things that would be quite impossible to see from the corresponding cross tables (too many numbers!)."

```{r echo=FALSE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE, fig.align="center"}
plot(tea_mca, invisible=c("ind"), habillage = "quali")
```
Or by using [this nice approach](http://www.gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/) to display both the observations and the categories. Moreover, since some individuals will be overlapped, we can add some density curves with geom_density2d() to see those zones that are highly concentrated:

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE, fig.align="center"}
 # MCA plot of observations and categories
cats = apply(dftea, 2, function(x) nlevels(as.factor(x)))

mca1_vars_df = data.frame(tea_mca$var$coord, Variable = rep(names(cats), cats))

ggplot(data = mca1_vars_df, aes(x = Dim.1, y = Dim.2)) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_point(colour = "gray50", alpha = 0.7) +
  geom_density2d(colour = "gray80") +
  geom_text(data = mca1_vars_df, 
            aes(x = Dim.1, y = Dim.2, 
                label = rownames(mca1_vars_df), colour = Variable)) +
  ggtitle("MCA plot of variables using My tea data") +
  scale_colour_discrete(name = "Variable")
```

On this biplot the first two dimensions are shown.Variable categories with a similar profile are grouped together. Negatively correlated variable categories are positioned on opposite sides of the plot origin (opposed quadrants).
We observe that there are a few categories located quie near to the center of the graph. Unpackaded tea and tea shops as well as tea bag and chain store categories are close to one another. Additionally, not work, alone and age category from 45 to 59 are located in one group.The youngest and the oldest respondents are the closest to each other with regard to the age groups, i.e. they have similar tea drinking habits.There seems to be one outlier category, those who drink tea with "other"" ways on the top of the plot. 
The first dimension captures mainly in what form people have their tea and where they buy it from. Individuals with high coordinates on the first component tend to by their tea in tea shops unpackaged and they are likely to drink it with lemon, whereas low coordinate-individuals buy tea bags in chain stores (more common~closer to the axis) and use milk.
For the second dimension there are "in-between" individuals at the top: they do their either unpacked and teabag shopping in either the chain stores and the tea shops, and cannot really say how they drink it and thus describe it using "other". 

References: 

-  [https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)

-  [http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/#biplot](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/114-mca-multiple-correspondence-analysis-in-r-essentials/#biplot)

-  [https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)

-  [http://factominer.free.fr/factomethods/categories-description.html](http://factominer.free.fr/factomethods/categories-description.html)
[http://factominer.free.fr/factomethods/dimensions-description.html](http://factominer.free.fr/factomethods/dimensions-description.html)

-  [http://factominer.free.fr/factomethods/multiple-correspondence-analysis.html](http://factominer.free.fr/factomethods/multiple-correspondence-analysis.html)

-  [http://www.gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/](http://www.gastonsanchez.com/visually-enforced/how-to/2012/10/13/MCA-in-R/)
